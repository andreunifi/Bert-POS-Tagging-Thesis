{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreunifi/Bert-POS-Tagging-Thesis/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uffr0VyJXtjm"
      },
      "source": [
        "[BERT](https://arxiv.org/abs/1810.04805) is known to be good at Sequence tagging tasks like Named Entity Recognition. Let's see if it's true for POS-tagging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMdNh4-5Xtjo"
      },
      "outputs": [],
      "source": [
        "__author__ = \"kyubyong\"\n",
        "__address__ = \"https://github.com/kyubyong/nlp_made_easy\"\n",
        "__email__ = \"kbpark.linguist@gmail.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WzwYwYLXtjp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9z4zu9LSXtjp",
        "outputId": "5fca2a0d-5801-4245-e87d-dc2e7c66b287"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPN1Kf7uXtjq"
      },
      "source": [
        "# Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-G2IZO0Xtjq"
      },
      "source": [
        "Thanks to the great NLTK, we don't have to worry about datasets. Some of Penn Tree Banks are included in it. I believe they serves for the purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTGk4RkWXtjq",
        "outputId": "88c4cff2-37e7-4529-b407-0e75dbe249d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3914"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "tagged_sents = nltk.corpus.treebank.tagged_sents()\n",
        "len(tagged_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0WfROVvXtjq"
      },
      "outputs": [],
      "source": [
        "tagged_sents[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBJbKJ4OXtjq"
      },
      "outputs": [],
      "source": [
        "tags = list(set(word_pos[1] for sent in tagged_sents for word_pos in sent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "crDFCjIEXtjr",
        "outputId": "f4496c23-769a-46d2-a960-800a10779177"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"#,,,VB,FW,$,CC,-RRB-,.,WDT,``,NNPS,RB,LS,CD,VBP,POS,DT,PRP,VBG,'',WRB,WP$,NNS,VBN,NNP,:,JJR,EX,IN,TO,RP,SYM,-NONE-,MD,UH,RBS,-LRB-,NN,PDT,WP,VBZ,JJ,RBR,VBD,PRP$,JJS\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\",\".join(tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnCI4bjgXtjr"
      },
      "outputs": [],
      "source": [
        "# By convention, the 0'th slot is reserved for padding.\n",
        "tags = [\"<pad>\"] + tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5R9XteVXtjr"
      },
      "outputs": [],
      "source": [
        "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
        "idx2tag = {idx:tag for idx, tag in enumerate(tags)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Vrg3S2FXtjr",
        "outputId": "107a80cf-49a2-4c6f-8eda-2a4b7b4cccc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3522, 392)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Let's split the data into train and test (or eval)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data = train_test_split(tagged_sents, test_size=.1)\n",
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1LXnS43eXtjs",
        "outputId": "b1f49403-eee6-44b5-b623-abb992113edf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr9Tjz8UXtjs"
      },
      "source": [
        "# Data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWppLYqNXtjs"
      },
      "outputs": [],
      "source": [
        "# Provare a cambiare i tokenizers e tracciare il comportamento come tempi ed accuratezza.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFAfGpnh1Tb8"
      },
      "source": [
        "# **PosDataset class extends Pythorch Dataset**\n",
        "\n",
        ">\n",
        "It looks like you're defining a custom dataset class, PosDataset, for part-of-speech tagging using BERT. Let's break down the key components of this class:\n",
        "\n",
        "Initialization (__init__ method):\n",
        "\n",
        "tagged_sents: The input parameter representing a list of tagged sentences. Each sentence is a list of tuples, where each tuple contains a word and its corresponding part-of-speech tag.\n",
        "Data Processing in Initialization:\n",
        "\n",
        "sents and tags_li: Lists to store tokenized sentences and their corresponding part-of-speech tags. The special tokens [CLS] and [SEP] are added to the beginning and end of each sentence.\n",
        "Tokenization is performed using the BERT tokenizer, and the tokenized sentences (sents) and part-of-speech tags (tags_li) are stored in the class.\n",
        "Length Method (__len__):\n",
        "\n",
        "Returns the number of sentences in the dataset.\n",
        "Get Item Method (__getitem__):\n",
        "\n",
        "Retrieves an item from the dataset by index.\n",
        "words and tags: Original words and part-of-speech tags for the current sentence.\n",
        "x, is_heads, and y: Lists for tokenized words, indicator of whether a token is the first piece of a word, and corresponding part-of-speech tag indices, respectively.\n",
        "Tokenization and conversion to indices are performed using the BERT tokenizer and the provided tag2idx mapping.\n",
        "The method returns the original words, tokenized word IDs (x), indicator for the first piece of each word (is_heads), original part-of-speech tags, part-of-speech tag IDs (y), and the sequence length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nfiRM9zXtjs"
      },
      "outputs": [],
      "source": [
        "class PosDataset(data.Dataset):\n",
        "    def __init__(self, tagged_sents):\n",
        "        sents, tags_li = [], [] # list of lists\n",
        "        for sent in tagged_sents:\n",
        "            words = [word_pos[0] for word_pos in sent]\n",
        "            tags = [word_pos[1] for word_pos in sent]\n",
        "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
        "            tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n",
        "        self.sents, self.tags_li = sents, tags_li\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
        "\n",
        "        # We give credits only to the first piece.\n",
        "        x, y = [], [] # list of ids\n",
        "        is_heads = [] # list. 1: the token is the first piece of a word\n",
        "        for w, t in zip(words, tags):\n",
        "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
        "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            is_head = [1] + [0]*(len(tokens) - 1)\n",
        "\n",
        "            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
        "            yy = [tag2idx[each] for each in t]  # (T,)\n",
        "\n",
        "            x.extend(xx)\n",
        "            is_heads.extend(is_head)\n",
        "            y.extend(yy)\n",
        "\n",
        "        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n",
        "\n",
        "        # seqlen\n",
        "        seqlen = len(y)\n",
        "\n",
        "        # to string\n",
        "        words = \" \".join(words)\n",
        "        tags = \" \".join(tags)\n",
        "        return words, x, is_heads, tags, y, seqlen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRPRlQNz9qGj"
      },
      "outputs": [],
      "source": [
        "dataset = PosDataset(tagged_sents)\n",
        "dataset[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mzi__QQ0fUa"
      },
      "source": [
        "# **Commento:**\n",
        "\n",
        "> Il tokenizer di Bert non mi è familiare, creo qui sotto una cella per effettuare, su una sentence di debug, una possibile esecuzione\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCZyPc7Z01cl",
        "outputId": "1d3b07de-f4dd-4809-aaa3-aad5294f1dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, -> ['Hello', ',']\n",
            "how -> ['how']\n",
            "does -> ['does']\n",
            "BERT -> ['B', '##ER', '##T']\n",
            "tokenizer -> ['token', '##izer']\n",
            "work? -> ['work', '?']\n",
            "Hello, how does BERT tokenizer work? -> ['[CLS]', 'Hello', ',', 'how', 'does', 'B', '##ER', '##T', 'token', '##izer', 'work', '?', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "# già importato from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "# già caricato in memoria tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize a sentence\n",
        "text = \"Hello, how does BERT tokenizer work?\"\n",
        "for t in text:\n",
        "  tokens = tokenizer.tokenize(t)\n",
        "  print(\"Tokens:\", tokens)\n",
        "\n",
        "# Convert tokens to IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Token IDs:\", token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AervRjjxM67_"
      },
      "source": [
        "# **Pad function**\n",
        "\n",
        "\n",
        "> The pad function appears to be a data processing function that pads the input batch to the length of the longest sequence in the batch. Let's break down the key components of this function:\n",
        "\n",
        "Input Parameters:\n",
        "\n",
        "> batch: A batch of samples, where each sample is a tuple containing information about words, tokenized word IDs (x), indicator for the first piece of each word (is_heads), original part-of-speech tags, part-of-speech tag IDs (y), and sequence length.\n",
        "Processing Steps:\n",
        "\n",
        ">Extract relevant information from the batch using lambda functions (f). f(0), f(2), f(3), and f(-1) extract words, indicator for the first piece of each word, original part-of-speech tags, and sequence lengths, respectively.\n",
        "\n",
        "Finds the maximum sequence length (maxlen) in the batch.\n",
        "\n",
        ">Define a lambda function f that pads sequences to a specified length (seqlen). This function is used to pad both the tokenized word IDs (x) and part-of-speech tag IDs (y). Padding is done with zeros (0), which likely corresponds to the <pad> token.\n",
        "\n",
        "Apply the padding function to the tokenized word IDs (x) and part-of-speech tag IDs (y) using the maximum sequence length (maxlen).\n",
        "\n",
        "Convert the padded tokenized word IDs (x) and part-of-speech tag IDs (y) to PyTorch LongTensors using torch.LongTensor.\n",
        "\n",
        ">Return the padded words, tokenized word IDs (x), indicator for the first piece of each word (is_heads), original part-of-speech tags, padded part-of-speech tag IDs (y), and sequence lengths.\n",
        "\n",
        ">In summary, this function is used to pad a batch of sequences to the length of the longest sequence in the batch, making it suitable for input to a neural network where all sequences in a batch must have the same length.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBXvNqr2Xtjs"
      },
      "outputs": [],
      "source": [
        "def pad(batch):\n",
        "    '''Pads to the longest sample'''\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    words = f(0)\n",
        "    is_heads = f(2)\n",
        "    tags = f(3)\n",
        "    seqlens = f(-1)\n",
        "    maxlen = np.array(seqlens).max()\n",
        "\n",
        "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
        "    x = f(1, maxlen)\n",
        "    y = f(-2, maxlen)\n",
        "\n",
        "\n",
        "    f = torch.LongTensor\n",
        "\n",
        "    return words, f(x), is_heads, tags, f(y), seqlens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QIuiBSWXtjs"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MF7RQj6KXtjs"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDti5W0uXtjt",
        "outputId": "555b3180-3f9b-4f40-e6d8-3f1cb44ef519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 48 µs, sys: 0 ns, total: 48 µs\n",
            "Wall time: 54.4 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size=None):\n",
        "        super().__init__()\n",
        "        # Provare a cambiare i modelli e tracciare il comportamento come tempi ed accuratezza.\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        # Qui puoi divertirti a cambiare o embedding size se si puo'\n",
        "        # o a cambiare la rete neurale di uscita, che adesso e' un\n",
        "        # semplice singolo layer linear.\n",
        "        self.fc = nn.Linear(768, vocab_size)\n",
        "        #self.fc = [nn.Dense(768, 512), nn.Linear(512, vocab_size)]\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: (N, T). int64\n",
        "        y: (N, T). int64\n",
        "        '''\n",
        "        x = x.to(self.device)\n",
        "        if self.training:\n",
        "            self.bert.train()\n",
        "            encoded_data = self.bert(x).last_hidden_state\n",
        "        else:\n",
        "            self.bert.eval()\n",
        "            with torch.no_grad():\n",
        "              encoded_data = self.bert(x).last_hidden_state\n",
        "        logits = self.fc(encoded_data)\n",
        "        y_hat = logits.argmax(-1)\n",
        "        return logits, y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMODm1q7Xtjt"
      },
      "source": [
        "# Train an evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXUTB5IVXtjt"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(iterator):\n",
        "        words, x, is_heads, tags, y, seqlens = batch\n",
        "        _y = y # for monitoring\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(x) # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
        "        y = y.view(-1)  # (N*T,)\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%10==0: # monitoring\n",
        "            print(\"step: {}, loss: {}\".format(i, loss.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuJrE2mWXtjt"
      },
      "outputs": [],
      "source": [
        "def eval(model, iterator):\n",
        "    model.eval()\n",
        "\n",
        "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            words, x, is_heads, tags, y, seqlens = batch\n",
        "\n",
        "            _, y_hat = model(x)  # y_hat: (N, T)\n",
        "\n",
        "            Words.extend(words)\n",
        "            Is_heads.extend(is_heads)\n",
        "            Tags.extend(tags)\n",
        "            Y.extend(y.numpy().tolist())\n",
        "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
        "\n",
        "    ## gets results and save\n",
        "    with open(\"result\", 'w') as fout:\n",
        "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
        "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
        "            preds = [idx2tag[hat] for hat in y_hat]\n",
        "            assert len(preds)==len(words.split())==len(tags.split())\n",
        "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
        "                fout.write(\"{} {} {}\\n\".format(w, t, p))\n",
        "            fout.write(\"\\n\")\n",
        "\n",
        "    ## calc metric\n",
        "    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
        "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
        "\n",
        "    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n",
        "\n",
        "    print(\"acc=%.2f\"%acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPlR_iKuXtjt"
      },
      "source": [
        "## Load model and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuUFPlKfXtjt",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "model = Net(vocab_size=len(tag2idx))\n",
        "model.to(device)\n",
        "model = nn.DataParallel(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hbO-UKJXtjt"
      },
      "outputs": [],
      "source": [
        "train_dataset = PosDataset(train_data)\n",
        "eval_dataset = PosDataset(test_data)\n",
        "\n",
        "train_iter = data.DataLoader(dataset=train_dataset,\n",
        "                             batch_size=8,\n",
        "                             shuffle=True,\n",
        "                             num_workers=1,\n",
        "                             collate_fn=pad)\n",
        "test_iter = data.DataLoader(dataset=eval_dataset,\n",
        "                             batch_size=8,\n",
        "                             shuffle=False,\n",
        "                             num_workers=1,\n",
        "                             collate_fn=pad)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for words, x, is_heads, tags, y, seqlen in train_iter:\n",
        "  print(model(x))"
      ],
      "metadata": {
        "id": "CmQJIW7HLlVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "4fT31pTwXtju",
        "outputId": "5c2485bd-62fe-4eac-9042-0e415a66d5c1",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, loss: 3.855541944503784\n",
            "step: 10, loss: 3.8497655391693115\n",
            "step: 20, loss: 3.848134994506836\n",
            "step: 30, loss: 3.854391574859619\n",
            "step: 40, loss: 3.859161138534546\n",
            "step: 50, loss: 3.887922525405884\n",
            "step: 60, loss: 3.899444818496704\n",
            "step: 70, loss: 3.8609259128570557\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-ea503cd9d832>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-159f4a83ab48>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(model, train_iter, optimizer, criterion)\n",
        "eval(model, test_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s3W3ez0Xtju"
      },
      "source": [
        "Check the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyF5X0s_Xtju",
        "outputId": "9ce9716a-5641-4c24-f1b8-fd4962593b98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Bonds NNS NNS',\n",
              " 'due JJ JJ',\n",
              " 'in IN IN',\n",
              " '2005 CD CD',\n",
              " 'have VBP VBP',\n",
              " 'a DT DT',\n",
              " '7 CD CD',\n",
              " '1\\\\/2 CD CD',\n",
              " '% NN NN',\n",
              " 'coupon NN NN',\n",
              " 'and CC CC',\n",
              " 'are VBP VBP',\n",
              " 'priced VBN VBN',\n",
              " '*-1 -NONE- -NONE-',\n",
              " 'at IN IN',\n",
              " 'par NN NN',\n",
              " '. . .',\n",
              " '',\n",
              " 'Mr. NNP NNP',\n",
              " 'Sidak NNP NNP',\n",
              " 'served VBD VBD',\n",
              " 'as IN IN',\n",
              " 'an DT DT',\n",
              " 'attorney NN NN',\n",
              " 'in IN IN',\n",
              " 'the DT DT',\n",
              " 'Reagan NNP NNP',\n",
              " 'administration NN NN',\n",
              " '. . .',\n",
              " '',\n",
              " 'Municipal NNP NNP',\n",
              " 'Issues NNPS NNPS',\n",
              " '',\n",
              " 'Viacom NNP NNP',\n",
              " 'denies VBZ VBZ',\n",
              " '0 -NONE- -NONE-',\n",
              " 'it PRP PRP',\n",
              " \"'s VBZ VBZ\",\n",
              " 'using VBG VBG',\n",
              " 'pressure NN NN',\n",
              " 'tactics NNS NNS',\n",
              " '. . .',\n",
              " '',\n",
              " 'Tokyo NNP NNP',\n",
              " \"'s POS POS\",\n",
              " 'leading VBG VBG',\n",
              " 'program NN NN',\n",
              " 'traders NNS NNS',\n",
              " 'are VBP VBP',\n",
              " 'the DT DT',\n",
              " 'big JJ JJ',\n",
              " 'U.S. NNP NNP',\n",
              " 'securities NNS NNS',\n",
              " 'houses NNS NNS',\n",
              " ', , ,',\n",
              " 'though IN IN',\n",
              " 'the DT DT',\n",
              " 'Japanese NNP NNS',\n",
              " 'are VBP VBP',\n",
              " 'playing VBG VBG',\n",
              " 'catch-up NN JJ',\n",
              " '. . .',\n",
              " '',\n",
              " 'That DT DT',\n",
              " \"'s VBZ VBZ\",\n",
              " 'why WRB WRB',\n",
              " 'Columbia NNP NNP',\n",
              " 'just RB RB',\n",
              " 'wrote VBD VBD',\n",
              " 'off RP RP',\n",
              " '$ $ $',\n",
              " '130 CD CD',\n",
              " 'million CD CD',\n",
              " '*U* -NONE- -NONE-',\n",
              " 'of IN IN',\n",
              " 'its PRP$ PRP$',\n",
              " 'junk NN NN',\n",
              " 'and CC CC',\n",
              " 'reserved VBD VBD',\n",
              " '$ $ $',\n",
              " '227 CD CD',\n",
              " 'million CD CD',\n",
              " '*U* -NONE- -NONE-',\n",
              " 'for IN IN',\n",
              " 'future JJ JJ',\n",
              " 'junk NN NN',\n",
              " 'losses NNS NNS',\n",
              " '*T*-1 -NONE- -NONE-',\n",
              " '. . .',\n",
              " '',\n",
              " 'Allergan NNP NNP',\n",
              " 'Inc. NNP NNP',\n",
              " 'said VBD VBD',\n",
              " '0 -NONE- -NONE-',\n",
              " 'it PRP PRP',\n",
              " 'received VBD VBD',\n",
              " 'Food NNP NNP',\n",
              " 'and CC CC',\n",
              " 'Drug NNP NNP',\n",
              " 'Administration NNP NNP']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "open('result', 'r').read().splitlines()[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWtp7c1JXtju"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
